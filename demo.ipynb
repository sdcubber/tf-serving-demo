{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF serving demo:\n",
    "* train a keras model with multiple outputs\n",
    "* serve the model\n",
    "    * compare timing with in-memory timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Store a text file for train generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def flatten(l): return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe\n",
    "df_train = pd.read_csv(\"./data/train_v2.csv\")\n",
    "\n",
    "# Make label maps\n",
    "labels = sorted(list(set(flatten([l.split(' ') for l in df_train['tags'].values]))))\n",
    "\n",
    "weather_labels = ['clear', 'cloudy', 'haze', 'partly_cloudy']\n",
    "ground_labels = [l for l in labels if l not in weather_labels]\n",
    "\n",
    "N_WLAB = len(weather_labels)\n",
    "N_GLAB = len(ground_labels)\n",
    "\n",
    "wlabel_map = {l: i for i, l in enumerate(weather_labels)}\n",
    "glabel_map = {l: i for i, l in enumerate(ground_labels)}\n",
    "\n",
    "def get_labels_binary(s, labelmap):\n",
    "    idx = [v for v in [labelmap.get(w) for w in s.split(' ')] if v != None]\n",
    "    labels = np.zeros(len(labelmap), dtype=np.int64)\n",
    "    labels[idx] = 1\n",
    "    return labels\n",
    "\n",
    "def array_to_str(arr):\n",
    "    return(str(arr.tolist()))\n",
    "df_train['wlabel'] = df_train['tags'].apply(get_labels_binary, args=(wlabel_map,))\n",
    "df_train['glabel'] = df_train['tags'].apply(get_labels_binary, args=(glabel_map,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a single instance without weather label -> remove it\n",
    "missing_w_idx = np.where(np.array([np.sum(v) for v in df_train['wlabel'].values]) != 1.0)[0][0]\n",
    "df_train = df_train.drop(index=missing_w_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map everything to strings'\n",
    "df_train['wlabel'] = df_train['wlabel'].map(array_to_str)\n",
    "df_train['glabel'] = df_train['glabel'].map(array_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as text file\n",
    "df_train.drop('tags', axis=1).to_csv('./data/TRAIN.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. write a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    \"\"\"Custom generator to train a keras model\n",
    "\n",
    "    Call .generate() to get the actual generator\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, batch_size):\n",
    "        self.fp = file_path\n",
    "        self.bs = batch_size\n",
    "\n",
    "        self._get_filesize()\n",
    "\n",
    "    def _get_filesize(self):\n",
    "        with open(self.fp, 'r') as f:\n",
    "            self.fz = sum(1 for line in f)\n",
    "        print(f'Found {self.fz} lines')\n",
    "\n",
    "    def _parse_line(self, line):\n",
    "        \"\"\"Parse a line from the data and return context ids, target id, label\"\"\"\n",
    "        *ctxt_ids, tgt_id, label = [int(n) for n in line.split(' ')]\n",
    "        return ctxt_ids, tgt_id, label\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"The actual generator\"\"\"\n",
    "        num_batches = int(self.fz / self.bs)\n",
    "        while True:\n",
    "            with open(self.fp, 'r') as f:\n",
    "                for i in range(num_batches - 1):\n",
    "                        x_c, x_t, y = [], [], []\n",
    "                        for j in range(self.bs):\n",
    "                            l = f.readline()\n",
    "                            c, t, l = self._parse_line(l)\n",
    "                            x_c.append(c)\n",
    "                            x_t.append(t)\n",
    "                            y.append(l)\n",
    "\n",
    "                        Xctx = np.array(x_c)\n",
    "                        Xtgt = np.array(x_t)\n",
    "                        Y = np.array(y)\n",
    "                        yield ([Xtgt, Xctx], Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import ast \n",
    "\n",
    "def randomHorizontalFlip(image, p=0.5):\n",
    "    \"\"\"Do a random horizontal flip with probability p\"\"\"\n",
    "    if np.random.random() < p:\n",
    "        image = np.fliplr(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def randomVerticalFlip(image, p=0.5):\n",
    "    \"\"\"Do a random vertical flip with probability p\"\"\"\n",
    "    if np.random.random() < p:\n",
    "        image = np.flipud(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"Custom generator\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, data_path, im_size, batch_size, shuffle, mode='train'):\n",
    "        self.fp = file_path  # path to training file\n",
    "        self.dp = data_path # path to raw images\n",
    "        self.imsz = im_size\n",
    "        self.bsz = batch_size\n",
    "        self.shuf = shuffle\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.df = pd.read_csv(self.fp)\n",
    "\n",
    "    def get_instance_indexes(self):\n",
    "        indexes = list(self.df.index)\n",
    "        if self.shuf:\n",
    "            np.random.shuffle(indexes)\n",
    "        return indexes\n",
    "\n",
    "    def get_batch_features(self, indexes):\n",
    "        batch_features = np.zeros((len(indexes), self.imsz, self.imsz, 3))\n",
    "\n",
    "        # Fill up container\n",
    "        for i, ix in enumerate(indexes):\n",
    "\n",
    "            im = load_img(os.path.join(self.dp, self.df['image_name'][ix] + '.jpg'))\n",
    "            w, h = im.size\n",
    "            resize = max(self.imsz / w, self.imsz / h)\n",
    "            # PIL .resize() requires size as (width, height)!\n",
    "            newdim = (int(w * resize), int(h * resize))\n",
    "            newim = im.resize(newdim)\n",
    "\n",
    "            im = img_to_array(newim)\n",
    "            im = preprocess_input(im)\n",
    "\n",
    "            if True:\n",
    "                im = randomHorizontalFlip(im)\n",
    "                im = randomVerticalFlip(im)\n",
    "\n",
    "            batch_features[i] = im\n",
    "\n",
    "            return batch_features\n",
    "\n",
    "    def get_batch_labels(self, indexes):\n",
    "        if self.mode == 'test':\n",
    "            return None\n",
    "        else:\n",
    "            # Empty containers for labels\n",
    "            w_labels, g_labels = np.zeros((len(indexes), N_WLAB)), np.zeros((len(indexes), N_GLAB))\n",
    "            # Fill up container\n",
    "            for i, ix in enumerate(indexes):\n",
    "                w_labels[i] = np.array(ast.literal_eval(\n",
    "                    self.df['wlabel'][ix]), dtype=int)\n",
    "                g_labels[i] = np.array(ast.literal_eval(\n",
    "                    self.df['glabel'][ix]), dtype=int)\n",
    "            return [w_labels, g_labels]\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "            indexes = self.get_instance_indexes()\n",
    "            num_batches = int(np.ceil(len(self.df) / self.bsz))\n",
    "            for i in range(num_batches):\n",
    "                if i == (num_batches - 1):\n",
    "                    batch_indexes = indexes[i * self.bsz:]\n",
    "                else:\n",
    "                    batch_indexes = indexes[i * self.bsz:(i + 1) * self.bsz]\n",
    "\n",
    "                X = self.get_batch_features(batch_indexes)\n",
    "                y = self.get_batch_labels(batch_indexes)\n",
    "                yield (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train a model\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Layer, Concatenate, Add, Subtract\n",
    "from keras.layers import BatchNormalization, Dropout, Activation\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape, Multiply, Dot\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Model\n",
    "from keras.applications import VGG16, ResNet50\n",
    "import keras.backend as K\n",
    "from keras import metrics\n",
    "\n",
    "class VGG_classifier(object):\n",
    "    def __init__(self, im_size, n_neurons, imagenet):\n",
    "        self.im_size = im_size\n",
    "        self.n_channels = 3\n",
    "        self.n_neurons = n_neurons\n",
    "        self.imagenet = imagenet\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        # Define input\n",
    "        self.x = Input(shape=(self.im_size, self.im_size, 3))\n",
    "\n",
    "        # Pretrained VGG16\n",
    "        weights = None\n",
    "        if self.imagenet:\n",
    "            weights = 'imagenet'\n",
    "        VGGmodel = VGG16(include_top=False, weights=weights,\n",
    "                         input_tensor=self.x,\n",
    "                         input_shape=(self.im_size, self.im_size, self.n_channels),\n",
    "                         pooling='max')\n",
    "\n",
    "        # VGG_out = Flatten()(VGGmodel.output) # in case of no pooling\n",
    "        VGG_out = VGGmodel.output\n",
    "        VGG_out = BatchNormalization()(VGG_out)\n",
    "        VGG_out = Dropout(0.25)(VGG_out)\n",
    "\n",
    "        # batchnorm + dense layers\n",
    "        fc_1 = BatchNormalization()(Dense(self.n_neurons, activation='relu')(VGG_out))\n",
    "        self.fc_1 = Dropout(0.25)(fc_1)\n",
    "        self.weather = Dense(N_WLAB, activation='softmax')(self.fc_1)\n",
    "        self.ground = Dense(N_GLAB, activation='sigmoid')(self.fc_1)\n",
    "        self.model = Model(inputs=self.x, outputs=[self.weather, self.ground])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG_classifier(48, 0, 128, True).model\n",
    "model.compile(optimizer='sgd', loss=['categorical_crossentropy', 'binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator('./data/TRAIN.csv', './data/train/', 48, 16, shuffle=True).generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  5/100 [>.............................] - ETA: 1:54 - loss: nan - dense_8_loss: nan - dense_9_loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-13a6729ef086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venvs/sbx3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbx3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbx3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbx3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbx3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbx3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/sbx3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=generator, steps_per_epoch=100, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (sandbox)",
   "language": "python",
   "name": "sbx3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
